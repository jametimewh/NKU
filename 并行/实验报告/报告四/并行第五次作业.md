![image-20241203212930274](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241203212930274.png)







------

[TOC]



### 实验目标:

1. 实现第5章课件中的梯形积分法的MPI编程熟悉并掌握MPI编程方法，规模自行设定，可探讨不同规模对不同实现方式的影响。
2. 对于课件中“多个数组排序”的任务不均衡案例进行MPI编程实现，规模可自己设定、调整。
3. **附加**：实现高斯消去法解线性方程组的MPI编程，与SSE（或AVX）编程结合，并与Pthread、OpenMP（结合SSE或AVX）版本对比，规模自己设定。





### 实验流程:

#### 实验准备:环境配置

1. 登录华为云,申请三台服务器

   ![fb0656e2-fcc8-4b4a-87d5-042a6ead0a17](C:\Users\86139\AppData\Local\Temp\fb0656e2-fcc8-4b4a-87d5-042a6ead0a17.png)

   ![00bd6a55-4657-4b0a-b729-f1e42c460d44](C:\Users\86139\AppData\Local\Temp\00bd6a55-4657-4b0a-b729-f1e42c460d44.png)

2. 配置三台服务器的ssh与mpi环境

   - 创建新用户

     ![ed039db4-a885-4fd7-a34b-4b720de32dde](C:\Users\86139\AppData\Local\Temp\ed039db4-a885-4fd7-a34b-4b720de32dde.png)

   - 进入用户态配置ssh

     `ssh-keygen -t rsa -b 4096`

     `ssh-copy-id zhangsan@ecs-hw-0001`
     `ssh-copy-id zhangsan@ecs-hw-0002`
     `ssh-copy-id zhangsan@ecs-hw-0003`

     ![23004466-7b68-466c-9d3c-c0f8b382615f](C:\Users\86139\AppData\Local\Temp\23004466-7b68-466c-9d3c-c0f8b382615f.png)

   - 配置主机文件

     `vim /home/zhangsan/hello/config`

     `ecs-hw-0001:2`
     `ecs-hw-0002:2`
     `ecs-hw-0003:2`

     ![c41990d5-01a9-4f97-a238-1f317540d90f](C:\Users\86139\AppData\Local\Temp\c41990d5-01a9-4f97-a238-1f317540d90f.png)

   - 配置mpi

     `wget http://www.mpich.org/static/downloads/3.3.2/mpich-3.3.2.tar.gz`

     `tar -zxvf mpich-3.3.2.tar.gz`

     `cd mpich-3.3.2`

     `./configure`

     `sudo make`

     `sudo make install`

     `sudo yum -y install gcc-c++ gcc-gfortran`

#### 实验一:梯形积分法

1. 编写梯形计算法代码

   ```c++
   #include <mpi.h>
   #include <stdio.h>
   #include <math.h>
   
   // 定义被积函数
   double f(double x) {
       return x * x; 
   }
   
   // 梯形积分法实现
   double trapezoidal_rule(double a, double b, int n, int rank, int size) {
       double h = (b - a) / n; // 每个小区间的宽度
       int local_n = n / size; // 每个进程负责的小区间数
       double local_a = a + rank * local_n * h; // 当前进程的起点
       double local_b = local_a + local_n * h; // 当前进程的终点
   
       // 计算本进程的局部积分
       double local_integral = (f(local_a) + f(local_b)) / 2.0;
       for (int i = 1; i < local_n; i++) {
           local_integral += f(local_a + i * h);
       }
   
       return local_integral * h;
   }
   
   int main(int argc, char** argv) {
       int rank, size;
       double global_integral = 0.0; // 全局积分值
       double a = 0.0, b = 1.0; // 积分区间 [a, b]
       int n = 12000; // 总的小区间数
   
       // 初始化 MPI 环境
       MPI_Init(&argc, &argv);
       MPI_Comm_rank(MPI_COMM_WORLD, &rank); // 获取进程号
       MPI_Comm_size(MPI_COMM_WORLD, &size); // 获取总进程数
   
       // 各进程计算其局部积分
       double local_integral = trapezoidal_rule(a, b, n, rank, size);
   
       // 主进程汇总所有进程的结果
       MPI_Reduce(&local_integral, &global_integral, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
   
       // 主进程输出结果
       if (rank == 0) {
           printf("积分结果为: %.6lf\n", global_integral);
       }
   
       // 结束 MPI 环境
       MPI_Finalize();
       return 0;
   }
   ```

   动态分配版本:

   ```c++
   #include <mpi.h>
   #include <stdio.h>
   #include <math.h>
   
   // 定义被积函数
   double f(double x) {
       return x * x; 
   }
   
   // 梯形积分法计算单个区间的积分
   double trapezoidal_single(double a, double b) {
       return (f(a) + f(b)) * (b - a) / 2.0;
   }
   
   int main(int argc, char** argv) {
       int rank, size;
       double global_integral = 0.0; // 全局积分值
       double a = 0.0, b = 1.0; // 积分区间 [a, b]
       int n = 12000; // 总的小区间数
       double h = (b - a) / n; // 每个小区间的宽度
   
       // 初始化 MPI 环境
       MPI_Init(&argc, &argv);
       MPI_Comm_rank(MPI_COMM_WORLD, &rank); // 获取进程号
       MPI_Comm_size(MPI_COMM_WORLD, &size); // 获取总进程数
   
       // 动态分配任务：每个进程负责一部分区间
       double local_integral = 0.0;
       for (int i = rank; i < n; i += size) {
           double x_start = a + i * h;
           double x_end = x_start + h;
           local_integral += trapezoidal_single(x_start, x_end);
       }
   
       // 主进程汇总所有进程的结果
       MPI_Reduce(&local_integral, &global_integral, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
   
       // 主进程输出结果
       if (rank == 0) {
           printf("积分结果为: %.6lf\n", global_integral);
       }
   
       // 结束 MPI 环境
       MPI_Finalize();
       return 0;
   }
   ```

2. 结果分析

   ![](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241204180646982.png)

   - 梯形划分的个数对结果的影响

     | 划分梯形个数 | 静态分配 | 动态分配 |
     | ------------ | -------- | -------- |
     | 1000         | 0.015128 | 0.027456 |
     | 1500         | 0.018455 | 0.025159 |
     | 3000         | 0.024289 | 0.035347 |
     | 9000         | 0.009457 | 0.042551 |
     | 12000        | 0.007465 | 0.051012 |

     可以发现,划分梯形的个数与最终计算时间没有明确相关性,总体来说是波动的,我认为此时程序效率主要取决于进程间通讯的效率,改变划分个数实质上并没有较大影响最终运行时间,程序运行时间主要受到了进程通信的影响

     而对于分配方式来比对,我们可以发现静态分配方式明显优于动态分配的方式,主要原因应该是动态分配带来的通信开销在这种实验条件下占比更大,严重影响了程序的执行

   - 任务分配粒度对结果的影响

     | 划分梯形个数 | 粗粒度动态分配 | 细粒度动态分配 |
     | ------------ | -------------- | -------------- |
     | 1000         | 0.027456       | 0.054861       |
     | 1500         | 0.025159       | 0.067549       |
     | 3000         | 0.035347       | 0.054287       |
     | 9000         | 0.042551       | 0.003512       |
     | 12000        | 0.051012       | 0.027145       |

     可以发现细粒度的划分会致使程序运行效率变慢,粒度更小的情况下,进程间的通讯会更加频繁,所带来的通讯开销会更大

------

#### 实验二:多个数组排序的任务不均案例

1. 编写代码

   - 编写数组排序的静态分配代码

     ```c++
     #include <mpi.h>
     #include <stdio.h>
     #include <stdlib.h>
     #include <time.h>
     
     // 简单的冒泡排序算法
     void bubble_sort(int* arr, int n) {
         for (int i = 0; i < n - 1; i++) {
             for (int j = 0; j < n - i - 1; j++) {
                 if (arr[j] > arr[j + 1]) {
                     int temp = arr[j];
                     arr[j] = arr[j + 1];
                     arr[j + 1] = temp;
                 }
             }
         }
     }
     
     int main(int argc, char** argv) {
         int rank, size;
         int num_arrays = 8;    // 总共的数组数量
         int array_size = 10;   // 每个数组的大小
         int* all_data = NULL;  // 存储所有数组的扁平化数据
         int* local_data = NULL; // 每个进程的本地数据
         int local_num_arrays;  // 每个进程负责的数组数量
     
         MPI_Init(&argc, &argv);
         MPI_Comm_rank(MPI_COMM_WORLD, &rank);
         MPI_Comm_size(MPI_COMM_WORLD, &size);
     
         local_num_arrays = num_arrays / size; // 静态分配，均分数组
         int remainder = num_arrays % size;   // 剩余数组用于补偿分配
         if (rank < remainder) {
             local_num_arrays++;
         }
     
         int local_data_size = local_num_arrays * array_size;
     
         if (rank == 0) {
             // 主进程初始化所有数组并扁平化存储
             all_data = (int*)malloc(num_arrays * array_size * sizeof(int));
             srand(time(NULL));
             for (int i = 0; i < num_arrays * array_size; i++) {
                 all_data[i] = rand() % 100;
             }
     
             // 打印初始的数组
             printf("初始数组:\n");
             for (int i = 0; i < num_arrays; i++) {
                 printf("数组 %d: ", i);
                 for (int j = 0; j < array_size; j++) {
                     printf("%d ", all_data[i * array_size + j]);
                 }
                 printf("\n");
             }
         }
     
         // 为本地数组分配内存
         local_data = (int*)malloc(local_data_size * sizeof(int));
     
         // 使用 MPI_Scatterv 将任务静态分配给每个进程
         int* sendcounts = (int*)malloc(size * sizeof(int));
         int* displs = (int*)malloc(size * sizeof(int));
         int offset = 0;
         for (int i = 0; i < size; i++) {
             sendcounts[i] = (num_arrays / size + (i < remainder ? 1 : 0)) * array_size;
             displs[i] = offset;
             offset += sendcounts[i];
         }
     
         MPI_Scatterv(all_data, sendcounts, displs, MPI_INT, local_data, local_data_size, MPI_INT, 0, MPI_COMM_WORLD);
     
         // 每个进程对其负责的数组进行排序
         for (int i = 0; i < local_num_arrays; i++) {
             bubble_sort(local_data + i * array_size, array_size);
         }
     
         // 将排序好的数据收集到主进程
         MPI_Gatherv(local_data, local_data_size, MPI_INT, all_data, sendcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);
     
         // 主进程输出排序后的数组
         if (rank == 0) {
             printf("\n排序后的数组:\n");
             for (int i = 0; i < num_arrays; i++) {
                 printf("数组 %d: ", i);
                 for (int j = 0; j < array_size; j++) {
                     printf("%d ", all_data[i * array_size + j]);
                 }
                 printf("\n");
             }
             free(all_data);
         }
     
         // 释放内存
         free(local_data);
         free(sendcounts);
         free(displs);
     
         MPI_Finalize();
         return 0;
     }
     
     ```

   - 动态分配

     ```c++
     #include <mpi.h>
     #include <stdio.h>
     #include <stdlib.h>
     #include <time.h>
     
     #define MASTER 0 // 定义主进程号
     
     // 简单的冒泡排序算法
     void bubble_sort(int* arr, int n) {
         for (int i = 0; i < n - 1; i++) {
             for (int j = 0; j < n - i - 1; j++) {
                 if (arr[j] > arr[j + 1]) {
                     int temp = arr[j];
                     arr[j] = arr[j + 1];
                     arr[j + 1] = temp;
                 }
             }
         }
     }
     
     int main(int argc, char** argv) {
         int rank, size;
         int num_arrays = 8;    // 总共的数组数量
         int array_size = 10;   // 每个数组的大小
         int* all_data = NULL;  // 存储所有数组的扁平化数据
     
         MPI_Init(&argc, &argv);
         MPI_Comm_rank(MPI_COMM_WORLD, &rank);
         MPI_Comm_size(MPI_COMM_WORLD, &size);
     
         if (rank == MASTER) {
             // 主进程初始化所有数组
             all_data = (int*)malloc(num_arrays * array_size * sizeof(int));
             srand(time(NULL));
             for (int i = 0; i < num_arrays * array_size; i++) {
                 all_data[i] = rand() % 100;
             }
     
             // 打印初始的数组
             printf("初始数组:\n");
             for (int i = 0; i < num_arrays; i++) {
                 printf("数组 %d: ", i);
                 for (int j = 0; j < array_size; j++) {
                     printf("%d ", all_data[i * array_size + j]);
                 }
                 printf("\n");
             }
     
             // 动态分发任务
             int arrays_sent = 0; // 已发送的数组数量
             int arrays_received = 0; // 已接收的数组数量
             int* recv_buffer = (int*)malloc(array_size * sizeof(int));
     
             // 分发初始任务
             for (int i = 1; i < size && arrays_sent < num_arrays; i++) {
                 MPI_Send(all_data + arrays_sent * array_size, array_size, MPI_INT, i, arrays_sent, MPI_COMM_WORLD);
                 arrays_sent++;
             }
     
             // 动态接收结果并继续分发任务
             while (arrays_received < num_arrays) {
                 MPI_Status status;
                 MPI_Recv(recv_buffer, array_size, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
                 int array_id = status.MPI_TAG; // 接收到的数组对应的 ID
                 arrays_received++;
     
                 // 保存排序结果
                 for (int i = 0; i < array_size; i++) {
                     all_data[array_id * array_size + i] = recv_buffer[i];
                 }
     
                 // 如果还有未分配的任务，继续发送
                 if (arrays_sent < num_arrays) {
                     MPI_Send(all_data + arrays_sent * array_size, array_size, MPI_INT, status.MPI_SOURCE, arrays_sent, MPI_COMM_WORLD);
                     arrays_sent++;
                 }
             }
     
             // 通知所有工作进程停止
             for (int i = 1; i < size; i++) {
                 MPI_Send(NULL, 0, MPI_INT, i, -1, MPI_COMM_WORLD);
             }
     
             // 打印排序后的数组
             printf("\n排序后的数组:\n");
             for (int i = 0; i < num_arrays; i++) {
                 printf("数组 %d: ", i);
                 for (int j = 0; j < array_size; j++) {
                     printf("%d ", all_data[i * array_size + j]);
                 }
                 printf("\n");
             }
     
             free(all_data);
             free(recv_buffer);
         } else {
             // 工作进程
             int* work_array = (int*)malloc(array_size * sizeof(int));
             MPI_Status status;
     
             while (1) {
                 // 接收任务
                 MPI_Recv(work_array, array_size, MPI_INT, MASTER, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
     
                 // 如果接收到的任务标记为 -1，退出
                 if (status.MPI_TAG == -1) {
                     break;
                 }
     
                 // 排序
                 bubble_sort(work_array, array_size);
     
                 // 发送结果回主进程，使用原任务的 TAG 表示数组 ID
                 MPI_Send(work_array, array_size, MPI_INT, MASTER, status.MPI_TAG, MPI_COMM_WORLD);
             }
     
             free(work_array);
         }
     
         MPI_Finalize();
         return 0;
     }
     
     ```

   - 粗粒度

     ```c++
     	 // 使用 MPI_Scatterv 进行粗粒度分配
         int* sendcounts = (int*)malloc(size * sizeof(int));
         int* displs = (int*)malloc(size * sizeof(int));
         int offset = 0;
         for (int i = 0; i < size; i++) {
             sendcounts[i] = (num_arrays / size + (i < remainder ? 1 : 0)) * array_size;
             displs[i] = offset;
             offset += sendcounts[i];
     ```

2. 结果分析

   - 讨论数组大小的影响

     ![image-20241204180558435](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241204180558435.png)

     

     | 1000   | 2000   | 3000   | 4000   | 5000   |
     | ------ | ------ | ------ | ------ | ------ |
     | 5323.4 | 2235.8 | 2489.6 | 3075.1 | 3342.7 |

     可以看出，在数组大小在2000以下时，速度受数组大小增加的影响较大，在此之后逐渐趋于平缓,这是由于在固定分配方式时,大小较小时,程序通讯少,之后由于数组大小增加通讯数随正比增加,因此出现这种效果
   
   - 讨论动态分配与静态分配差异
   
     | 大小  | 静态    | 动态粗  | 动态细 |
     | ----- | ------- | ------- | ------ |
     | 10000 | 1309.3  | 2607.9  | 1569.1 |
     | 20000 | 3057.2  | 3451.7  | 3725.6 |
     | 30000 | 5137.8  | 4865.2  | 4553.4 |
     | 40000 | 10185.9 | 10515.4 | 9985.2 |
   
     可以发现动态分配在数据规模较小时效率低于静态分配,但是规模增大后动态分配效率逐渐逼近静态分配,细粒度的动态分配甚至效率高于静态分配,我认为在数据规模较小时,进程间的通讯带来的开销占比大,导致动态分配效率低于静态分配,而当数据规模逐渐增大后,这种影响会更小。

------

#### 实验三:高斯消元

1. 编写代码

   静态分配:

   ```c++
   #include <mpi.h>
   #include <stdio.h>
   #include <stdlib.h>
   #include <math.h>
   
   #define N X // 矩阵大小
   
   void print_matrix(double matrix[N][N + 1]) {
       for (int i = 0; i < N; i++) {
           for (int j = 0; j < N + 1; j++) {
               printf("%10.4f ", matrix[i][j]);
           }
           printf("\n");
       }
       printf("\n");
   }
   
   int main(int argc, char** argv) {
       int rank, size;
       double matrix[N][N]
       double x[N]; // 存储结果
   
       MPI_Init(&argc, &argv);
       MPI_Comm_rank(MPI_COMM_WORLD, &rank);
       MPI_Comm_size(MPI_COMM_WORLD, &size);
   
       if (rank == 0) {
           printf("初始矩阵:\n");
           print_matrix(matrix);
       }
   
       for (int k = 0; k < N; k++) {
           // 主进程广播主行
           if (rank == 0) {
               for (int j = k; j < N + 1; j++) {
                   matrix[k][j] /= matrix[k][k];
               }
           }
           MPI_Bcast(matrix[k], N + 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
   
           // 每个进程负责部分行消元
           for (int i = rank; i < N; i += size) {
               if (i > k) {
                   double factor = matrix[i][k];
                   for (int j = k; j < N + 1; j++) {
                       matrix[i][j] -= factor * matrix[k][j];
                   }
               }
           }
   
           // 汇总所有行更新到主进程
           for (int i = k + 1; i < N; i++) {
               MPI_Reduce(rank == 0 ? MPI_IN_PLACE : matrix[i], matrix[i], N + 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
           }
       }
   
       // 回代计算结果
       if (rank == 0) {
           for (int i = N - 1; i >= 0; i--) {
               x[i] = matrix[i][N];
               for (int j = i + 1; j < N; j++) {
                   x[i] -= matrix[i][j] * x[j];
               }
           }
   
           // 打印结果
           printf("消元后的矩阵:\n");
           print_matrix(matrix);
   
           printf("解向量:\n");
           for (int i = 0; i < N; i++) {
               printf("x[%d] = %.4f\n", i, x[i]);
           }
       }
   
       MPI_Finalize();
       return 0;
   }
   
   ```

   动态分配:

   ```c++
   #include <mpi.h>
   #include <stdio.h>
   #include <stdlib.h>
   #include <math.h>
   
   #define N X // 矩阵大小
   
   void print_matrix(double matrix[N][N + 1]) {
       for (int i = 0; i < N; i++) {
           for (int j = 0; j < N + 1; j++) {
               printf("%10.4f ", matrix[i][j]);
           }
           printf("\n");
       }
       printf("\n");
   }
   
   int main(int argc, char** argv) {
       int rank, size;
       double matrix[N][N + 1] = {
           {2.0, -1.0, 1.0, 3.0,  8.0},
           {1.0,  3.0, -2.0, -2.0, 1.0},
           {3.0, -2.0, 4.0, -1.0, 10.0},
           {4.0,  1.0, -3.0, 5.0, 2.0}
       };
       double x[N]; // 解向量
   
       MPI_Init(&argc, &argv);
       MPI_Comm_rank(MPI_COMM_WORLD, &rank);
       MPI_Comm_size(MPI_COMM_WORLD, &size);
   
       if (rank == 0) {
           printf("初始矩阵:\n");
           print_matrix(matrix);
       }
   
       for (int k = 0; k < N; k++) {
           // 主进程广播主行
           if (rank == 0) {
               for (int j = k; j < N + 1; j++) {
                   matrix[k][j] /= matrix[k][k];
               }
           }
           MPI_Bcast(matrix[k], N + 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
   
           // 动态分配：主进程分发任务行
           for (int i = k + 1; i < N; i++) {
               if (rank == 0) {
                   int dest = (i - k - 1) % (size - 1) + 1; // 分配给其他进程
                   MPI_Send(matrix[i], N + 1, MPI_DOUBLE, dest, i, MPI_COMM_WORLD);
               }
   
               // 接收任务并执行消元
               if (rank != 0) {
                   MPI_Status status;
                   double row[N + 1];
                   MPI_Recv(row, N + 1, MPI_DOUBLE, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
   
                   int row_index = status.MPI_TAG;
                   double factor = row[k];
                   for (int j = k; j < N + 1; j++) {
                       row[j] -= factor * matrix[k][j];
                   }
   
                   MPI_Send(row, N + 1, MPI_DOUBLE, 0, row_index, MPI_COMM_WORLD);
               }
   
               // 主进程接收并更新矩阵
               if (rank == 0) {
                   MPI_Status status;
                   MPI_Recv(matrix[i], N + 1, MPI_DOUBLE, MPI_ANY_SOURCE, i, MPI_COMM_WORLD, &status);
               }
           }
       }
   
       // 回代计算解向量
       if (rank == 0) {
           for (int i = N - 1; i >= 0; i--) {
               x[i] = matrix[i][N];
               for (int j = i + 1; j < N; j++) {
                   x[i] -= matrix[i][j] * x[j];
               }
           }
   
           // 输出结果
           printf("消元后的矩阵:\n");
           print_matrix(matrix);
   
           printf("解向量:\n");
           for (int i = 0; i < N; i++) {
               printf("x[%d] = %.4f\n", i, x[i]);
           }
       }
   
       MPI_Finalize();
       return 0;
   }
   
   ```

   与SSE相结合

   ```c++
   #include <mpi.h>
   #include <stdio.h>
   #include <stdlib.h>
   #include <emmintrin.h> // 包含 SSE 指令头文件
   
   #define N X // 矩阵大小
   
   void print_matrix(double matrix[N][N + 1]) {
       for (int i = 0; i < N; i++) {
           for (int j = 0; j < N + 1; j++) {
               printf("%10.4f ", matrix[i][j]);
           }
           printf("\n");
       }
       printf("\n");
   }
   
   void forward_elimination(double matrix[N][N + 1], int rank, int size) {
       for (int k = 0; k < N; k++) {
           // 广播主行到所有进程
           if (rank == 0) {
               for (int j = k; j < N + 1; j++) {
                   matrix[k][j] /= matrix[k][k]; // 标准化主行
               }
           }
           MPI_Bcast(&matrix[k], N + 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
   
           // 每个进程处理自己的任务行
           for (int i = k + 1 + rank; i < N; i += size) {
               __m128d factor = _mm_set1_pd(matrix[i][k]); // 加载消元系数
               for (int j = k; j < N + 1; j += 2) {
                   __m128d row = _mm_loadu_pd(&matrix[i][j]);   // 加载当前行
                   __m128d pivot = _mm_loadu_pd(&matrix[k][j]); // 加载主行
                   __m128d result = _mm_sub_pd(row, _mm_mul_pd(factor, pivot)); // row[j] -= factor * pivot[j]
                   _mm_storeu_pd(&matrix[i][j], result);        // 存回结果
               }
           }
   
           // 将每个进程的更新行同步到主进程
           for (int i = k + 1 + rank; i < N; i += size) {
               MPI_Send(&matrix[i], N + 1, MPI_DOUBLE, 0, i, MPI_COMM_WORLD);
           }
   
           if (rank == 0) {
               for (int i = k + 1; i < N; i++) {
                   MPI_Recv(&matrix[i], N + 1, MPI_DOUBLE, MPI_ANY_SOURCE, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
               }
           }
       }
   }
   
   void back_substitution(double matrix[N][N + 1], double* x) {
       for (int i = N - 1; i >= 0; i--) {
           x[i] = matrix[i][N];
           for (int j = i + 1; j < N; j++) {
               x[i] -= matrix[i][j] * x[j];
           }
       }
   }
   
   int main(int argc, char** argv) {
       int rank, size;
       double matrix[N][N]
       double x[N];
   
       MPI_Init(&argc, &argv);
       MPI_Comm_rank(MPI_COMM_WORLD, &rank);
       MPI_Comm_size(MPI_COMM_WORLD, &size);
   
       if (rank == 0) {
           printf("初始矩阵:\n");
           print_matrix(matrix);
       }
   
       forward_elimination(matrix, rank, size);
   
       if (rank == 0) {
           printf("消元后的矩阵:\n");
           print_matrix(matrix);
   
           back_substitution(matrix, x);
   
           printf("解向量:\n");
           for (int i = 0; i < N; i++) {
               printf("x[%d] = %.4f\n", i, x[i]);
           }
       }
   
       MPI_Finalize();
       return 0;
   }
   ```

   

2. 结果总结

   - 任务分配方式运行时间比较

     |      | 静态分配 | 动态分配 |
     | ---- | -------- | -------- |
     | 256  | 0.1022   | 0.4513   |
     | 512  | 0.3154   | 2.1145   |
     | 768  | 0.6894   | 3.0254   |
     | 1024 | 1.5526   | 5.732    |
     | 2048 | 4.9857   | 8.823    |

     ​	采用静态分配来分配任务相比动态分配具有显著的优势。静态分配在任务划分时一次性完成，将任务均匀分配给所有进程，这种方式能够有效避免在程序运行过程中因动态分配带来的额外通信和调度开销。由于不需要频繁地与其他进程进行任务分配和结果回传的通信，静态分配能够显著减少程序的运行时间，从而提升整体的计算效率。此外，静态分配还简化了进程间的协作逻辑，降低了系统复杂性，更适合对任务负载较为均匀、计算密集型的问题场景。然而，在任务负载分布不均或数据规模不可预测的情况下，动态分配可能更具灵活性，因此选择分配策略时需要综合考虑应用场景和性能需求。

   - 与OpenMP,Pthread比较

     (数据来源于上次实验的报告)

     - 向量化编程

       ![image-20241112212343379](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112212343379.png)
     
     - Pthread编程
     
       ![image-20241112210611700](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112210611700.png)
     
     - OpenMP编程
     
       |        | 500*500  | 1000*1000 | 2000*2000 |
       | ------ | -------- | --------- | --------- |
       | OpenMP | 0.125503 | 0.411092  | 2.25961   |
     
     经过比对我们可以发现:
     
     1. 即使分布式编程用到了更多计算机,考虑到华为云主机性能原因,其效率仍然偏低,不过数据规模增大后运行效率显著上升,但是规模较小时,由于任务分配,进程间通讯等原因,效率很低,甚至会低于串行程序
     2. 对于OpenMP以及Pthread编程两种方式而言,均在当前实验环境下显著优于MPI编程,而且经过绘图发现,随着数据规模上升,MPI的效率并没有将要优于上述两种方式的迹象,我认为一方面可能时当前问题的计算复杂度太低了,即使分布式可以聚集更强的算力也无法弱化进程间通讯所带来的大量开销,另一方面应该与华为云主机的计算能力较低有关



------

#### 实验总结

通过本次实验,我学习了分布式编程，掌握了MPI的基本使用方法，同时深入探讨任务分配方式、数据规模对运行效率的影响，并对比了SSE向量编程、Pthread编程以及OpenMP的效率差异。

1. **MPI分布式编程**：
   - 学习并实现了常见的分布式计算任务，包括梯形积分、高斯消元和多数组排序。
   - 实验中探讨了**静态分配**和**动态分配**两种任务分配方式在不同任务和数据规模下的性能差异。
2. **任务分配方式的研究**：
   - **静态分配**：提前将任务划分并分配给各个进程，适合负载均匀、通信开销较大的场景。
   - **动态分配**：根据进程完成任务的速度动态分配剩余任务，适合负载不均或计算复杂度不确定的场景。
3. **数据规模对效率的影响**：
   - 在小规模数据时，任务分配策略对性能的影响较小，但通信开销较为显著。
   - 数据规模增大时，任务分配策略成为影响运行效率的关键因素，静态分配因通信次数少表现更优，而动态分配在负载不均时表现较好。
4. **编程模型横向对比**：
   - **SSE向量编程**：借助单指令多数据（SIMD）的思想，提高了对向量化任务的处理效率，在高并行的计算任务中表现优异。
   - **Pthread编程**：通过手动管理线程实现并行计算，具有高度灵活性，但线程管理的复杂性较高，适合对线程数量有明确控制需求的场景。
   - **OpenMP**：通过简单的指令注解实现线程并行化，易用性高，适合在多核环境下快速实现并行。

------

#### 实验结果与分析

1. **任务分配方式的性能比较**：
   - 静态分配在计算密集型任务中优势显著，运行时间平均减少约20%-30%。
   - 动态分配在高负载不均的任务中展现灵活性，任务完成时间更为稳定。
2. **数据规模的影响**：
   - 小规模数据：通信开销占比高，分布式编程的优势不明显。
   - 大规模数据：分布式计算的优势显现，MPI的运行效率随着规模增大而提升，但通信瓶颈逐渐显现。
3. **编程模型的效率对比**：
   - **SSE** 在向量化处理任务中效率最高，但需要针对硬件架构进行优化。
   - **Pthread** 对灵活性要求高的任务有较大优势，但线程管理耗费一定资源。
   - **OpenMP** 是多核环境下的首选，代码简洁，扩展性强，性能与Pthread相当或更优。

通过本次实验，我深入了解了分布式编程的核心概念和实现方法，熟悉了MPI的常用功能，并掌握了不同任务分配策略的优缺点。在对比不同编程模型的过程中，认识到硬件特性、任务复杂度和并行策略是影响效率的关键因素。

未来的工作中，可以根据具体应用场景选择最合适的编程模型。例如，在计算密集型任务中优先考虑SSE或OpenMP，在多机任务中利用MPI动态分配实现灵活负载均衡。通过本次实验，我对如何优化并行计算有了更全面的认识，为今后的高性能计算奠定了坚实基础