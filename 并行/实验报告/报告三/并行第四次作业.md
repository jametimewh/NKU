<p style='text-align:center;font-size:20px'>并行第四次作业</p>

<p style='text-align:center;font-size:20px'>2212534 魏思诚</p>

### 实验目标:

1. 分别实现课件中的梯形积分法的Pthread、OpenMP版本，熟悉并掌握OpenMP编程方法，探讨两种编程方式的异同。
2. 对于课件中“多个数组排序”的任务不均衡案例进行OpenMP编程实现（规模可自己调整），并探索不同循环调度方案的优劣。提示：可从任务分块的大小、线程数的多少、静态动态多线程结合等方面进行尝试，探索规律。
3. **附加题：**实现高斯消去法解线性方程组的OpenMP编程，与SSE/AVX编程结合，并探索优化任务分配方法。

### 实验流程:

#### 实验一:

分别实现课件中的梯形积分法的Pthread、OpenMP版本，熟悉并掌握OpenMP编程方法，探讨两种编程方式的异同

1. 编写实现梯形积分法的OpenMP版本:

   ```c++
   #include <stdio.h>
   #include <stdlib.h>
   #include <omp.h>
   #include <chrono>
   #include<iostream>
   void Trap(double a, double b, int n, double* global_result_p);
   double f(double x) {
       return 2.548 * x + 1.05;
   }
   
   int main(int argc, char* argv[]) {
       if (argc < 2) {
           fprintf(stderr, "Usage: %s <number of threads>\n", argv[0]);
           exit(1);
       }
   
       double global_result = 0.0;
       double a, b;
       int n;
       int thread_count = strtol(argv[1], NULL, 10);
   
       printf("Enter a, b, and n\n");
       scanf("%lf %lf %d", &a, &b, &n);
       auto start = std::chrono::high_resolution_clock::now();
   
       #pragma omp parallel num_threads(thread_count)
       {
           Trap(a, b, n, &global_result);
       }
       auto end = std::chrono::high_resolution_clock::now();
       std::chrono::duration<double> duration = end - start;
       std::cout << "time: " << duration.count() << "s" << std::endl;
       printf("With n = %d trapezoids, our estimate\n", n);
       printf("of the integral from %f to %f = %.14e\n", a, b, global_result);
       std::cin.get();
       return 0;
   }
   
   void Trap(double a, double b, int n, double* global_result_p) {
       double h, x, my_result;
       double local_a, local_b;
       int i, local_n;
   
       int my_rank = omp_get_thread_num();
       int thread_count = omp_get_num_threads();
   
       h = (b - a) / n;
       local_n = n / thread_count;
       local_a = a + my_rank * local_n * h;
       local_b = local_a + local_n * h;
   
       my_result = (f(local_a) + f(local_b)) / 2.0;
       for (i = 1; i <= local_n; i++) {
           x = local_a + i * h;
           my_result += f(x);
       }
       my_result = my_result * h;
   
       #pragma omp critical
       *global_result_p += my_result;
   }
   ```

   测试参数:

   `a=0 b=100 n=100000000`

   ![image-20241112192807694](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112192807694.png)

2. 编写实现梯形积分法的Pthread版本:

   ```c++
   #include <stdio.h>
   #include <stdlib.h>
   #include <pthread.h>
   #include <chrono>
   #include <iostream>
   #include <cstdint>  // 添加这个头文件
   
   void* Trap(void* rank);
   double f(double x) {
       return 2.548 * x + 1.05;
   }
   
   // 全局变量
   double global_result = 0.0;
   double a, b;
   int n;
   int thread_count;
   pthread_mutex_t mutex;
   
   int main(int argc, char* argv[]) {
       if (argc < 2) {
           fprintf(stderr, "Usage: %s <number of threads>\n", argv[0]);
           exit(1);
       }
   
       thread_count = strtol(argv[1], NULL, 10);
       pthread_t* thread_handles = (pthread_t*)malloc(thread_count * sizeof(pthread_t));
       
       printf("Enter a, b, and n\n");
       scanf("%lf %lf %d", &a, &b, &n);
   
       // 初始化互斥锁
       pthread_mutex_init(&mutex, NULL);
   
       auto start = std::chrono::high_resolution_clock::now();
   
       // 创建线程
       for (intptr_t thread = 0; thread < thread_count; thread++) {
           pthread_create(&thread_handles[thread], NULL, Trap, (void*)thread);
       }
   
       // 等待所有线程完成
       for (intptr_t thread = 0; thread < thread_count; thread++) {
           pthread_join(thread_handles[thread], NULL);
       }
   
       auto end = std::chrono::high_resolution_clock::now();
       std::chrono::duration<double> duration = end - start;
       std::cout << "time: " << duration.count() << "s" << std::endl;
       printf("With n = %d trapezoids, our estimate\n", n);
       printf("of the integral from %f to %f = %.14e\n", a, b, global_result);
   
       // 销毁互斥锁
       pthread_mutex_destroy(&mutex);
       free(thread_handles);
   
       std::cin.get();
       return 0;
   }
   
   void* Trap(void* rank) {
       intptr_t my_rank = (intptr_t)rank;  // 使用 intptr_t 类型
       double h = (b - a) / n;
       int local_n = n / thread_count;
       double local_a = a + my_rank * local_n * h;
       double local_b = local_a + local_n * h;
       double my_result = (f(local_a) + f(local_b)) / 2.0;
   
       for (int i = 1; i <= local_n; i++) {
           double x = local_a + i * h;
           my_result += f(x);
       }
       my_result *= h;
   
       // 使用互斥锁保护对 global_result 的更新
       pthread_mutex_lock(&mutex);
       global_result += my_result;
       pthread_mutex_unlock(&mutex);
   
       return NULL;
   }
   
   ```

   测试参数:

   `a=0 b=100 n=100000000`

   ![image-20241112193448517](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112193448517.png)

3. 探讨差异:

   1. 编程模型
      - **OpenMP**：基于**指令式**的并行编程模型。程序员通过在代码中插入特定的编译指令（如 `#pragma omp parallel`）来提示编译器将某些代码并行化。OpenMP 适合在已有的串行代码上快速添加并行化。
      - **Pthread**：基于**线程创建与管理**的模型，提供了细粒度的控制。程序员需要显式地创建、管理和同步线程，适合那些需要高度定制和细节控制的并行任务。
   2. 并行化的控制级别
      - **OpenMP**：控制简单。通过指令（如 `#pragma omp parallel for`）就可以实现并行循环，且编译器会自动处理线程的创建、分配和管理。OpenMP 对数据共享和同步有默认设置，可以轻松并行化计算密集型代码段。
      - **Pthread**：控制精细。程序员需要自己实现线程的创建、同步（如使用 `pthread_mutex` 进行互斥锁控制）、资源分配和销毁。这种精细控制允许实现更复杂的并行策略，但编程较复杂，适合有多种并行需求的程序。
   3. 数据共享与同步
      - **OpenMP**：提供了一些简单的机制来管理数据共享和同步。例如可以指定某些变量是共享（`shared`）还是私有（`private`）的，也可以使用 `#pragma omp critical` 来保证对共享数据的访问安全性。OpenMP 自动处理大多数同步需求，编写代码更简单。
      - **Pthread**：所有同步都需要显式定义。必须使用 `pthread_mutex`、`pthread_cond` 等工具来管理数据的一致性和线程的同步。此外，共享数据的访问需要手动同步，增加了代码复杂性，但灵活性也更高。
   4. 线程管理
      - **OpenMP**：自动管理线程池。程序员只需要设定并行化的指令，OpenMP 会根据系统资源自动调整线程数量、调度策略等。这使得 OpenMP 非常适合并行循环和任务分割较为均匀的情况。
      - **Pthread**：需要手动创建和管理线程。程序员可以精确地控制每个线程的生命周期、优先级和调度策略，但这增加了代码的复杂性和编写成本。适合需要严格线程控制的应用程序。
   5. 平台兼容性
      - **OpenMP**：标准化的并行编程接口，通常在所有支持 OpenMP 的编译器上都可以运行，如 GCC、Clang、Intel C++ 编译器等，跨平台支持较好。
      - **Pthread**：是 POSIX 标准的一部分，主要在类 UNIX 系统（如 Linux、macOS）上原生支持。Windows 需要第三方库的支持，如 pthreads-windows，但移植性比 OpenMP 差。

   

   #### 实验二:

   对于课件中“多个数组排序”的任务不均衡案例进行OpenMP编程实现（规模可自己调整），并探索不同循环调度方案的优劣。提示：可从任务分块的大小、线程数的多少、静态动态多线程结合等方面进行尝试，探索规律。

   1. 编写openMP版本数组排序:

      ```c++
      #include <iostream>
      #include <algorithm>
      #include <vector>
      #include <ctime>
      #include <immintrin.h>
      #include <windows.h>
      #include <omp.h>
      
      using namespace std;
      
      const int ARR_NUM = 10000;
      const int ARR_LEN = 10000;
      const int THREAD_NUM = 4;
      vector<int> arr[ARR_NUM];
      long long head, tail, freq; // timers
      
      void init(void) {
          srand(static_cast<unsigned>(time(NULL)));
          for (int i = 0; i < ARR_NUM; i++) {
              arr[i].resize(ARR_LEN);
              for (int j = 0; j < ARR_LEN; j++) {
                  arr[i][j] = rand();
              }
          }
      }
      
      void arr_sort_openmp() {
          int task_size = 50;
          QueryPerformanceCounter((LARGE_INTEGER*)&head);  // 记录并行计算开始的时间
      
          #pragma omp parallel num_threads(THREAD_NUM)
          {
              int thread_id = omp_get_thread_num();
              
              while (true) {
                  int task;
                  
                  // 分配任务编号并更新全局任务变量
                  #pragma omp critical
                  {
                      static int next_task = 0;
                      task = next_task;
                      next_task += task_size;
                  }
                  
                  if (task >= ARR_NUM) break;
      
                  int end = min(task + task_size, ARR_NUM); // 确保不会越界
                  for (int i = task; i < end; i++) {
                      stable_sort(arr[i].begin(), arr[i].end());
                  }
              }
      
              #pragma omp critical
              {
                  QueryPerformanceCounter((LARGE_INTEGER*)&tail); // 记录每个线程的结束时间
                  printf("Thread %d finished in %lf ms.\n", thread_id, (tail - head) * 1000.0 / freq);
              }
          }
      }
      
      int main(int argc, char* argv[]) {
          QueryPerformanceFrequency((LARGE_INTEGER*)&freq); // 获取计时器频率
          init();
      
          // 使用 OpenMP 并行执行排序
          arr_sort_openmp();
      
          return 0;
      }
      ```

   2. 对比之前编写的SSE版本效率

      1. 对比静态与动态和guided分配方式差异

         10000*10000大小 4线程

         SSE版本静态分配

         ![image-20241105192248178](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241105192248178.png)

         openMP版本动态分配(手动实现的)

         ![image-20241112195645071](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112195645071.png)

         

         改用自带的动态分配方式

         ```c++
          #pragma omp parallel num_threads(THREAD_NUM)
             {
                 // 任务分配: 使用 dynamic 调度
                 #pragma omp for schedule(dynamic, 10) // 这里使用了动态调度，每次分配10个任务
                 for (int i = 0; i < ARR_NUM; i++) {
                     stable_sort(arr[i].begin(), arr[i].end());
                 }
         
                 // 输出执行时间
                 #pragma omp critical
                 {
                     QueryPerformanceCounter((LARGE_INTEGER *)&tail);
                     printf("Thread %d: %lfms.\n", omp_get_thread_num(), (tail - head) * 1000.0 / freq);
                 }
             }
         ```

         ![image-20241112200831150](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112200831150.png)

         改用guided分配方式:

         ![image-20241112201208313](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112201208313.png)

         对比发现在数据规模一致的情况下,guided配方式的效率最高,而静态分配方式和动态分配方式的openmp编程效率都略大于向量化编程的效率,其中动态分配的效率也大于静态分配方式,这可能是由于数据分配更平均后,实现负载均衡,程序拥有更好的效率

      2. 比较数据规模不同时的效率

         1. 5000*5000 4线程

            ![image-20241112203226616](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112203226616.png)

         2. 20000*20000 4线程

            ![image-20241112203236710](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112203236710.png)

         3. 10000*10000 4 线程

            ![image-20241112203258864](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112203258864.png)

         经过比对发现,静态分配的劣势在数据集越大时越明显,这可能是由于规模增大后,负载不均的情况更加明显,而动态分配和guided分配的效率差异不明显,但是规模较小的时候guided更高效

#### 实验三(附加题):

实现高斯消去法解线性方程组的OpenMP编程，与SSE/AVX编程结合，并探索优化任务分配方法。

1. 编写高斯消元OpenMP版本:

   ```c++
   #include <iostream>
   #include <iomanip>
   #include <random>
   #include <chrono>
   #include <omp.h>
   
   const int N1 = 500;
   const int N2 = 1000;
   const int N3 = 2000;
   const int THREAD_COUNT = 8;
   
   void generateMatrix(float** matrix, int size) {
       std::random_device rd;
       std::mt19937 gen(rd());
       std::uniform_real_distribution<> dis(1.0, 100.0);
   
       for (int i = 0; i < size; ++i)
           for (int j = 0; j < size; ++j)
               matrix[i][j] = dis(gen);
   }
   
   // OpenMP 版本的高斯消元
   void gaussianEliminationOpenMP(float** matrix, int size) {
       #pragma omp parallel for
       for (int k = 0; k < size; ++k) {
           // 主元化
           float pivot = matrix[k][k];
           for (int j = k; j < size; ++j) {
               matrix[k][j] /= pivot;
           }
   
           #pragma omp parallel for
           for (int i = k + 1; i < size; ++i) {
               float factor = matrix[i][k];
               for (int j = k; j < size; ++j) {
                   matrix[i][j] -= factor * matrix[k][j];
               }
           }
       }
   }
   
   // 回代过程
   void backSubstitution(float** matrix, float* solution, int size) {
       for (int i = size - 1; i >= 0; --i) {
           solution[i] = matrix[i][size];
           for (int j = i + 1; j < size; ++j) {
               solution[i] -= matrix[i][j] * solution[j];
           }
           solution[i] /= matrix[i][i];
       }
   }
   
   int main() {
       int sizes[] = {N1, N2, N3};
   
       for (int size : sizes) {
           std::cout << "Matrix size: " << size << "x" << size << "\n";
   
           // 初始化矩阵
           float** matrix = new float*[size];
           for (int i = 0; i < size; ++i) {
               matrix[i] = new float[size + 1]; // 增加一列来存放结果
           }
   
           // 生成随机矩阵
           generateMatrix(matrix, size);
   
           // 使用 OpenMP 进行高斯消元
           auto start = std::chrono::high_resolution_clock::now();
           gaussianEliminationOpenMP(matrix, size);
           auto end = std::chrono::high_resolution_clock::now();
           std::chrono::duration<double> elapsed = end - start;
           std::cout << "Multithreaded Gaussian Elimination Time: " << elapsed.count() << " seconds\n";
   
           // 初始化解向量
           float* solution = new float[size];
   
           // 回代
           start = std::chrono::high_resolution_clock::now();
           backSubstitution(matrix, solution, size);
           end = std::chrono::high_resolution_clock::now();
           elapsed = end - start;
           std::cout << "Back Substitution Time: " << elapsed.count() << " seconds\n";
   
           // 释放矩阵内存
           for (int i = 0; i < size; ++i) {
               delete[] matrix[i];
           }
           delete[] matrix;
           delete[] solution;
   
           std::cout << "\n";
       }
       return 0;
   }
   
   ```

   与pthread版本比较:

   1. 比较规模不一定时,表现差异

      - const int N1 = 500;const int N2 = 1000;const int N3 = 2000;const int THREAD_COUNT = 8;

        openMP版本:

        ![image-20241112204441369](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112204441369.png)

        Phread版本

        ![image-20241112210710299](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112210710299.png)

        AVX版本

        

      - 经过对比发现,在数据集越大的情况下,openMP编程方式的优越性越不明显,pthread之所以在规模较小时效率低,可能原因是:
        1. **小规模任务**：当任务较小（例如矩阵较小）时，线程创建、调度以及线程之间的同步和通信开销可能占用了大量的时间。在这种情况下，OpenMP 在创建和销毁线程时的开销可能比较小，因为它内置的线程池机制能够复用线程
        2. **大规模任务**：当任务规模增大时，虽然线程创建的开销依然存在，但计算负载显著增大，线程的计算时间占据了更多的比例，这使得线程管理的开销变得相对较小。因此，pthread 的线程创建和销毁开销变得更为显著，尤其是当使用大量线程时，pthread 的管理开销可能大于 OpenMP 所隐式处理的调度机制
        3. **OpenMP**：在 OpenMP 中，线程池机制会自动进行任务划分（如果设置了调度策略），并且通过静态、动态或引导调度来处理负载均衡。对于较小的任务，OpenMP 的调度可以更灵活地分配任务到线程上，从而降低负载不均的影响。
        4. **pthread**：在 pthread 中，线程的任务划分通常需要程序员手动处理（例如，通过计算每个线程处理的任务范围），而且 pthread 没有像 OpenMP 那样的内置负载均衡机制。对于小任务，负载不均衡的影响较小，但随着任务规模增大，如果任务分配不均匀，pthread 可能出现性能瓶颈。

   2. 比较线程数目不同时的差异

      - Thread_Count=4:

        openmp编程:

        ![image-20241112211051323](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112211051323.png)

        pthread编程:

        ![image-20241112210611700](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112210611700.png)

        ![image-20241112212343379](C:\Users\86139\AppData\Roaming\Typora\typora-user-images\image-20241112212343379.png)

      - 线程数目不同时,我发现openmp效率虽然降低了,但是劣势不明显,但pthread编程效率出现比较特殊的情况:当数据规模小时,4线程pthread的效率反而高于8线程,这时,程序在线程的创建上的开销占比较大,更多线程的并行并没有让程序效率更高,而数据规模增大后,4线程的pthread效率低于了8线程,这时,线程创建消耗的资源占比小了,更多线程并行的优势明显,而两种方式效率都大于简单的向量编程



### 实验总结:

​	通过这次实验，我深入理解了多种并行编程技术在不同任务中的表现差异。OpenMP、pthread和AVX向量化在不同代码、数据规模和线程数目上展现出不同的效率特性。在高斯消元这样的数值计算密集型任务中，AVX向量化技术显著提升了处理速度，充分发挥了现代CPU的SIMD指令集优势。而在多数组排序这类内存访问模式较为复杂的任务中，OpenMP和pthread则提供了较好的线程并行性，尤其是在数据规模较大时，能够有效减少单线程的计算负担。

​	通过比较不同线程数目的影响，我意识到线程数的选择不仅依赖于硬件的核心数，还受到任务本身特性的制约。例如，在小规模数据上，过多的线程可能会带来线程管理的开销，导致性能下降；而在大规模数据上，合理增加线程数则能显著提高计算效率。

​	总的来说，选择合适的并行化技术和线程数需要结合具体的任务特性和硬件配置，才能获得最优的性能表现。这次实验不仅加深了我对并行编程的理解，也为今后在实际应用中选择最合适的优化策略提供了宝贵的经验。